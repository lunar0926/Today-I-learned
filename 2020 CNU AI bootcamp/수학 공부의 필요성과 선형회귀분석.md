## 막연하게 느껴졌던 수학

*  앞으로 머신러닝을 공부해야겠다고 마음 먹은 뒤로 여러 가지 정보를 찾아보았다. '머신러닝, 딥러닝에는 선형대수학, 확률과 통계 등의 과목을 공부해야 한다.' 라는 글과 영상은 정말 많이 봤지만 사실 나는 수학 공부를 하는 것이 막연하게 느껴졌다. 그야말로 무엇을, 어떻게, 왜 해야하는지 알 수 없어 막막했다. 

    이번 충남대 AI 부트캠프에서 머신러닝을 압축적으로 배우면서 모든 내용을 다 알 수도 없고 깊이 있게 배우지 않아 내가 얻어가는 것에는 한계가 있다고 생각하지만 머신러닝과 수학이 어떻게 쓰이는 지에 대해 대략적으로 알 수 있었고 앞으로 수학 공부를 어떻게 해나가야할지도 좀 더 명확해졌다. 그래서 내가 강의를 들을때 선형회귀분석을 이해하기위해 찾았던 수학 내용을 여기에 정리하려고 한다. 
 
 ## 선형 회귀 분석(Linear Regression)

 * 지도학습은 input값과 output값을 미리 주고 두 값 사이 관계를 통해서 관계식을 도출하여 결과적으로는 예측을 하는 모델을 만드는 것이 목표이다. 

    input과 output의 관계는 독립변수와 종속변수, x와 y 사이의 관계식이라고도 할 수 있다. 

    이렇게 관계식을 통해서 임의의 독립변수에 따른 종속변수를 구할 수 있는 예측을 하는 것을 선형 회귀라고 한다.

    독립 변수 x가 1개일 때를 단순 선형 회귀라고 한다.
    독립 변수 x가 여러 개일때는 다중 선형 회귀이다.

*  단순 선형 회귀의 경우에서,

    주어진 데이터로부터 x와 y의 관계식을 세우는 것이 가설(hypothesis) 이다.

    H(x)=Wx+b  (W= weight 가중치, b=bias 편향)

    적절한 W와 b 값을 찾아서 x와 y의 관계를 가장 잘 표현하는 가설 함수를 찾아야 한다.

    이 가설 함수는 일차 함수이고 직선의 방정식이다. 

* 좋은 **Hypothesis**를 찾는 것이 선형 회귀의 목적이라고 할 수 있는데 좋은 hypothesis는 실제값과 예측값의 차이를 통해서 알 수 있다.

    실제값과 예측값의 오차를 **Cost**라고 한다. 이 Cost가 낮을 수록 좋은 Hypothesis인 것이다.

    Cost 함수도 실제값과 예측값의 오차를 가장 잘 줄이는 식이어야 하고 문제에 따라 적합한 Cost 함수가 있다고 한다. 내가 생활코딩 머신러닝 강의에서 봤던 **MSE**는 회귀 문제에 주로 사용되는 평균 제곱 오차였다.

    위에서 hypothesis는 일차 함수, 직선이라고 하였다. 각 x에서의 y 값들은 점으로 표현된다.  x와 y의 관계를 가장 잘 나타내는 hypothesis를 찾는다는 것은 모든 점들과 가장 가까운 직선(hypothesis)을 구한다는 것이다. 오차는 예측값인 H(x)와 실제값인 점들의 거리 차이를 말한다. 

* 다시 정리하자면,

    예측을 잘하려는 것

    = X와 Y의 관계를 잘 찾으려는 것

    = Cost가 낮은 Hypothesis를 찾으려는 것

* 선형 회귀에서 학습하는 과정이란 

    1. 무작위 값을 통해서 hypothesis를 찾는다.
    2. 실제값과 예측값의 오차를 통해서 cost를 계산한다.
    3. cost가 낮아지는 방향으로 hypothesis를 업데이트한다.

    로 볼 수 있다.

* cost는 hypothesis에 의해 결정된다. 

    hypothesis가 정확하면(실제값과 예측값 사이의 거리가 비교적 가까우면) cost는 낮고, hypothesis가 부정확하면(실제값과 예측값 사이의 거리가 비교적 멀면) cost는 높다.

    cost와 hypothesis와의 관계로 인해서 cost는 hypothesis에 대한 함수라고 볼 수 있다.

    hypothesis를 정한다는 것은 그 값을 구하기 위한 함수를 만들고 weight와 bias를 정했다는 말이다.

    hypothesis는 weight에 의해 결정된다. weight가 정확하면 hypothesis도 정확하다.

    weight가 정확하면 cost는 낮고, weight가 부정확하면 cost는 높다고 할 수 있다.

    = cost는 weight에 대한 함수이다.

    강사님은 선형회귀를 

    "cost가 가장 낮을 때의 weight를 찾는 과정!" 이라고 정리하셨다.

* 위에서 쓴 선형 회귀에서 학습하는 과정을 수정하면 

    1. weight 초기값을 설정한다. (weight 초기값을 통해 hypothesis를 구하고 예측값을 통해 실제값과의 차이인 cost를 계산할 수 있음.
    2. 첫 번째 cost를 계산한다.
    3. cost가 낮아지는 방향으로 weight를 업데이트하여 cost가 가장 낮을 때의 weight를 찾는다.

* 잠깐 weight에 대해서 더 살펴보자면,

    H(x)=Wx+b 이 식에서 weight에 해당하는 W는 y값을 결정하는데 있어 큰 영향력을 가진다.

    가중치는 해당 x값의 중요도를 의미한다고 볼 수 있고,

    cost가 가장 낮을 때의 weight를 찾는다는 것은 x값의 가장 정확한 중요도를 찾는다는 말과 같다.

* 그러면 여기서 cost가 낮아지는 방향으로 weight를 업데이트하여 cost가 가장 낮을 때의 weight를 찾는다고 했는데 cost가 낮아지는 방향을 어떻게 알고, weight를 어떻게 수정해야 하는 것일까?

    이렇게 cost가 가장 낮을 때의 weight를 찾고, 결과적으로 좋은 hypothesis를 찾는 과정을 학습(train)이라고 하는데 이때 **경사하강법 알고리즘**이 사용된다.

    경사하강법에 대한 설명을 간략하게 들으면서 나는 미분이 바로 떠올랐다. 여기서 미분이 사용되는 것이다!

    최초의 직선은 H(x) = W(x)로 둔다. (W의 초기값 설정)

    Cost 함수는 이차함수로 나타내고  W에서의 접선의 기울기를 구한다. (여기서 **편미분**이라는 용어를 사용하는데 나는 편미분을 알지못한다.)

    이 기울기가 작아지는 방향으로 W를 수정한다.

    cost함수의 다음 지점을 결정하기 위해서 W에 기울기를 뺀다. (이때 음의 기울기일때 빼면 cost함수의 기울기가 0에 수렴하는 방향으로 w값이 정해진다. 양의 기울기더라도 빼면 0에 수렴하는 방향으로 w값이 정해진다.)

    편미분에 대한 내용은 따로 정리하기로 한다. 

    W값을 구하는 식에서 학습 계수도 나오는데 학습 계수는 cost가 점차 작아질 수 있도록 학습량을 조절해주는 것이라고 생각할 수 있다.

    기울기에 학습률 또는 보폭이라 불리는 스칼라를 곱하여 다음 w를 정하는 것이다.

    학습 계수가 작으면 학습 시간이 오래 걸릴 것이고 계수가 너무 크면 cost가 가장 낮을 때의 범위를 이탈할 수도 있다. 그래서 적절한 학습 계수를 둬야 한다.

    cost 함수가 복잡해지면 cost는 여전히 높은데 gradient가 0에 가까워지는 경우도 있다.

    = local minimum

    이 경우에는 optimizer (최적화)를 통해서 더 복잡한 식으로 해결한다.

## 참고

[https://wikidocs.net/](https://wikidocs.net/)

AI 부트캠프 머신러닝 강의 내용
